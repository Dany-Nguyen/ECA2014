\documentclass[a4paper,11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{multicol} 		%% Pour faire plusieurs colonnes
% Title Page
\title{Extraction des connaissances}
\author{Carbonnel Jessie \and Nguyen Daniel \and Pibre Lionel}
%\includegraphics[height=.15\textheight]{img/um2.jpg}\\[1cm]

\begin{document}
\maketitle

\section{Outils utilisés}

\begin{itemize}
	\item Hadoop
	\item d3JS
	\item TreeTagger
	\item API Weka
	\item Langage de programmation: Java
\end{itemize}
\section{Prétraitement}
\subsection{TreeTagger}
Afin de réaliser le prétraitement sur notre corpus, nous avons utilisé TreeTagger. Ce logiciel nous permet de connaître la classe grammaticale des mots et d'obtenir la forme lemmatisée de ces derniers. Cet outils nous a permis de nettoyer le corpus en supprimant par exemple les pronoms et les déterminants.\\

\begin{table}[!h]
\centering
 \begin{tabular}{|c|c|c|}
 \hline
    dès&PRP&dès\\
    que&KON&que\\
    je&PRO:PER&je\\
    lance&VER:pres&lancer\\
    l'&DET:ART&le\\
    application&NOM&application\\
    ça&PRO:DEM&cela\\
    marche&NOM&marche\\
    pas&ADV&pas\\
    sinon&KON&sinon\\
    j'&PRO:PER&je\\
    adore&VER:pres&adorer\\
    cyprien&ADJ&cyprien\\
    \dots&\dots&\dots\\
 \hline
 \end{tabular}
 \caption{Exemple de TreeTagger}
\end{table}

\subsection{Construction des fichier arff}
Nous avons décidé de créer huit fichiers arff différents afin de voir l'impact qu'a la composition du corpus sur les algorithmes de classification.

\begin{itemize}
 \item Le texte brut (sans aucune modification)
 \item Le texte brut en utilisant la forme lemmatisée des mots
 \item Le texte uniquement composé d'adjectifs, de noms et de verbes
 \item Le texte uniquement composé d'adjectifs, de noms et de verbes en utilisant la forme lemmatisée des mots
 \vspace{0.5cm}
 \item Ensuite, nous avons repris ces quatre fichiers arff mais en corrigeant l'orthographe des verbes aimer et adorer, nous avons aussi remplacer ``kiffer'' par aimer.
\end{itemize}

\section{Résultats et analyse}

\subsection{Les types d'algorithmes utilisés}

\begin{description}
 \item [NaiveBayes: ] la classification naïve bayésienne est un type de classification probabiliste simple basée sur le théorème de Bayes avec une forte indépendance des 
 hypothèses. \\
 
 \item [Arbre de décision: ] cet algorithme utilise une structure d'arbre. L'extrémité de chaque branche représente les différents résultats possibles 
 en fonction des décisions prises à chaque étape. Cet algorithme  répartit une population d'individus en groupes homogènes, selon un ensemble 
 de variables discriminantes en fonction d'un objectif fixé et connu.\\
 
 \item [SVM: ] Les SVM sont des classificateurs qui reposent sur deux idées clés:
 
\begin{itemize}
 \item La notion de marge maximale. La marge est la distance entre la frontière de séparation et les échantillons les plus proches. 
Ces derniers sont appelés vecteurs supports. Dans les SVM, la frontière de séparation est choisie comme celle qui maximise la marge. 
Le problème est de trouver la frontière séparatrice optimale, à partir d'un ensemble d'apprentissage. Cependant il existe déjà des algorithmes pour résoudre ce 
problème
 \item  Transformer l'espace de représentation des données d'entrées en un espace de plus grande dimension, dans lequel il est probable qu'il existe une 
 séparatrice linéaire. \\
 
\end{itemize}
 \item [Méthode par règles d'association: ] La méthode par règles d’association s’apparente à celle des arbres de décision. Chaque règle pouvant être 
 vue comme le parcours dans un arbre de décision binaire. Cependant les arbres de décisions essayent de réduire le nombre de parcours à explorer tandis 
 qu’ici on explore tout.\\
 
 \item [K plus proche voisin: ] L’algorithme KNN figure parmi les plus simples algorithmes d’apprentissage artificiel. Dans un contexte de classification d’une 
 nouvelle observation x, l’idée fondatrice simple est de faire voter les plus proches voisins de cette observation. La classe de x est déterminée en fonction de la 
 classe majoritaire parmi les k plus proches voisins de l’observation x. La méthode KNN est donc une méthode à base de voisinage, non-paramétrique ; Ceci signifiant 
 que l’algorithme permet de faire une classification sans faire d’hypothèse sur la fonction $y=f(x_1,x_2,…x_p)$ qui relie la variable dépendante aux variables indépendantes. 
\end{description}


\subsection{Résultats des instances correctement classifiées}

\subsubsection{Texte non corrigé}

\begin{minipage}{0.5\textwidth}
\begin{tabular}{|c|c|c|}
\hline
 & Binaire & Fréquence \\
 \hline
 NaiveBayes & 64.1788 & 63.6551 \\
 \hline
 SMO & 73.1201 & 73.6438 \\
 \hline
 IBk & 65.563 & 64.3659 \\
 \hline
 J48 & 67.8451 & 67.8638 \\
 \hline
 JRip & 64.7961 & 65.2637 \\
 \hline
\end{tabular}
\caption{Texte brut}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{tabular}{|c|c|c|}
\hline
 & Binaire & Fréquence \\
 \hline
 NaiveBayes & 65.8997 & 65.376 \\
 \hline
 SMO & 73.6438 & 74.3172 \\
 \hline
 IBk & 65.9371 & 64.4407 \\
 \hline
 J48 & 68.9113 & 68.7991 \\
 \hline
 JRip & 67.7703 & 68.3315 \\
 \hline
\end{tabular}
\caption{Texte brut lemmatisé}
\end{minipage}

\vspace{1cm}

\begin{minipage}{0.5\textwidth}
\begin{tabular}{|c|c|c|}
\hline
 & Binaire & Fréquence \\
 \hline
 NaiveBayes & 66.517 & 65.4321 \\
 \hline
 SMO & 72.5776 & 72.5963 \\
 \hline
 IBk & 64.5342 & 64.1227 \\
 \hline
 J48 & 69.2854 & 69.0797 \\
 \hline
 JRip & 66.33 & 65.9559 \\
 \hline
\end{tabular}
\caption{Texte nettoyé}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{tabular}{|c|c|c|}
\hline
 & Binaire & Fréquence \\
 \hline
 NaiveBayes & 67.9012 & 66.4796 \\
 \hline
 SMO & 72.8021 & 73.2697 \\
 \hline
 IBk & 65.9746 & 62.1212 \\
 \hline
 J48 & 69.0423 & 68.9862 \\
 \hline
 JRip & 67.5832 & 67.5645 \\
 \hline
\end{tabular}
\caption{Texte nettoyé lemmatisé}
\end{minipage}

\subsubsection{Texte corrigé}

\begin{minipage}{0.5\textwidth}
\begin{tabular}{|c|c|c|}
\hline
 & Binaire & Fréquence \\
 \hline
 NaiveBayes & 64.2724  & 63.6177 \\
 \hline
 SMO & 73.1949 & 73.569 \\
 \hline
 IBk & 65.6004 & 64.3659 \\
 \hline
 J48 & 67.8077 & 67.8451 \\
 \hline
 JRip & 65.2263 & 65.7127 \\
 \hline
\end{tabular}
\caption{Texte brut}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{tabular}{|c|c|c|}
\hline
 & Binaire & Fréquence \\
 \hline
 NaiveBayes & 66.0307 & 65.5256 \\
 \hline
 SMO & 73.4755 & 74.6914 \\
 \hline
 IBk & 66.0307 & 64.3285 \\
 \hline
 J48 & 69.5099 & 69.2667 \\
 \hline
 JRip & 67.4336 & 67.3588 \\
 \hline
\end{tabular}
\caption{Texte brut lemmatisé}
\end{minipage}

\vspace{1cm}

\begin{minipage}{0.5\textwidth}
\begin{tabular}{|c|c|c|}
\hline
 & Binaire & Fréquence \\
 \hline
 NaiveBayes & 66.835 & 65.4134 \\
 \hline
 SMO & 72.1848 & 72.4654 \\
 \hline
 IBk & 64.7774 & 64.7026 \\
 \hline
 J48 & 69.3229 & 69.3042 \\
 \hline
 JRip & 66.3113 & 65.9746 \\
 \hline
\end{tabular}
\caption{Texte nettoyé}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{tabular}{|c|c|c|}
\hline
 & Binaire & Fréquence \\
 \hline
 NaiveBayes & 67.8077 & 66.7228 \\
 \hline
 SMO & 73.6064 & 73.251 \\
 \hline
 IBk & 64.4407 & 63.3184 \\
 \hline
 J48 & 69.2854 & 69.5099 \\
 \hline
 JRip & 67.7142 & 67.3588 \\
 \hline
\end{tabular}
\caption{Texte nettoyé lemmatisé}
\end{minipage}


\subsection{Analyse des résultats}
Comment expliqué que les résultats obtenus soient aussi bas?

Lorsque nous avons fait le prétraitement, nous avons vu que deux éléments risquaient de limiter nos résultats.

\begin{description}
	\item [Les fautes d'orthographe et de frappe: ]``Je kiff grave car jadore cyprien et jaimefai lui poser un question : est ce que tu connais squeezie ( ca c oui c sur ) norman kihouu tal blackm ....''\\
	
	Dans ce cas, peu de mots sont reconnus par TreeTagger. Par exemple les mots ``kiff'' et ``jadore'' ne sont pas reconnus alors qu'ils déterminent l'opinion de l'utilisateur.
	
	\item [Les commentaires en anglais: ]Certains commentaires sont écrits en anglais et ne sont donc pas reconnus par TreeTagger.
\end{description}

On peut voir d'après les résultats obtenus que l'algorithme de classification le plus efficace est SMO. Cet algorithme est très robuste au bruit et comme on a pu le constater au vu des résultats, aux fautes d'orthographe.\\

Les algorithmes JRip et J48 ne sont pas efficacent car ils se basent sur les mots pour créer des règles afin de déterminer une classe à un commentaire. En effet, les fautes d'orthographes empêchent de faire des règles précises et efficaces car un même mot peut être écrit de plusieurs manières différentes dans les commentaires.
On peut aussi voir que peu importe si les mots sont représentés par la présense ou l'occurence, les résultats diffèrent très peu. \'{E}tant donné que notre corpus contient un nombre incalculable de fautes d'orthographe, la représentation des données n'impacte pas les résultats.\\

NaiveBayes est un algorithme de classification probabiliste, encore une fois les fautes d'orthographe jouent un rôle important dans les résultats de cet algorithme. En effet, les fautes l'empêchent de créer des caractéristiques spécifiques aux différentes classes, puisque les mêmes mots sont écrits de différentes façon dans le corpus.\\

Pour l'algorithme IBk, le fait que des mots apparaissent dans des classes différentes (par exemple le mot ``dommage'', qui apparaît dans toutes les classes), cela empêche d'obtenir de meilleurs résultats. De plus on peut remarquer que cet algorithme est plus efficace lorsque la représentation des mots est binaires, cela vient du fait que les mots vides ont moins d'importance lorsque les données sont binaires.

\section{Perspectives}
Utiliser des outils de TALN afin de corriger le corpus ainsi que traduire les commentaires en anglais. En effet, on peut remarquer dans les résultats que juste le fait de corriger l'orthographe de trois verbes, les algorithmes obtiennent pour la majorité d'entre eux de meilleurs résultats.\\


Tester d'autres algorithmes afin de voir s'il n'y a pas un ou plusieurs algorithmes plus performant que ceux testés au cours de ce projet.
\end{document}          
